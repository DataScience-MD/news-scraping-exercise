{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MST698S_news-scraping-exercise.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxERh/K1bpWRugFDZnFh8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurpleDin0/news-scraping-exercise/blob/master/Execution_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jIVlByg4YiE",
        "colab_type": "text"
      },
      "source": [
        "# MST 698S - Data Science Tools And Techniques \n",
        "# Bad Ozone Grasshoppers - News Scrapper Exercise\n",
        "\n",
        "Summary: \n",
        "\n",
        "Usage Details: This Notebook is desigened to be run in the [Google Colab environment](https://colab.research.google.com/). However, it should work in any Jupyter Notebooks or Jupyter Lab environment.  The main purpose of the notebook is to install relevant python libraries, execute the web scrapping code, and save the output to a cloud repository.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIUL-n5N6Q3i",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the Environment \n",
        "1. Clone the github repo [located here](https://github.com/PurpleDin0/news-scraping-exercise).  \n",
        "2. Install all required dependancies.  This is best done by storing all dependices to a `requirements.txt` in the github repo file and running a `pip install` for using that file.  \n",
        "```\n",
        "!pip install -r requirements.txt\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAx6SKzRtQ_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "d8ccc47b-ef9d-4121-da64-a2ef1f21f907"
      },
      "source": [
        "#First clone the relevant github repo\n",
        "!git clone https://github.com/PurpleDin0/news-scraping-exercise.git\n",
        "#Navigate to the newly created repo folder\n",
        "%cd /content/news-scraping-exercise\n",
        "#install all required python libraries\n",
        "!pip install -r requirements.txt\n",
        "# Once installed make sure to restart the runtime"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'news-scraping-exercise'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:   7% (1/14)\u001b[K\rremote: Compressing objects:  14% (2/14)\u001b[K\rremote: Compressing objects:  21% (3/14)\u001b[K\rremote: Compressing objects:  28% (4/14)\u001b[K\rremote: Compressing objects:  35% (5/14)\u001b[K\rremote: Compressing objects:  42% (6/14)\u001b[K\rremote: Compressing objects:  50% (7/14)\u001b[K\rremote: Compressing objects:  57% (8/14)\u001b[K\rremote: Compressing objects:  64% (9/14)\u001b[K\rremote: Compressing objects:  71% (10/14)\u001b[K\rremote: Compressing objects:  78% (11/14)\u001b[K\rremote: Compressing objects:  85% (12/14)\u001b[K\rremote: Compressing objects:  92% (13/14)\u001b[K\rremote: Compressing objects: 100% (14/14)\u001b[K\rremote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 15 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:   6% (1/15)   \rUnpacking objects:  13% (2/15)   \rUnpacking objects:  20% (3/15)   \rUnpacking objects:  26% (4/15)   \rUnpacking objects:  33% (5/15)   \rUnpacking objects:  40% (6/15)   \rUnpacking objects:  46% (7/15)   \rUnpacking objects:  53% (8/15)   \rUnpacking objects:  60% (9/15)   \rUnpacking objects:  66% (10/15)   \rUnpacking objects:  73% (11/15)   \rUnpacking objects:  80% (12/15)   \rUnpacking objects:  86% (13/15)   \rUnpacking objects:  93% (14/15)   \rUnpacking objects: 100% (15/15)   \rUnpacking objects: 100% (15/15), done.\n",
            "/content/news-scraping-exercise\n",
            "Requirement already satisfied: selenium==3.141.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.141.0)\n",
            "Collecting beautifulsoup4==4.8.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl\n",
            "Collecting requests==2.22.0\n",
            "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
            "Collecting lxml==4.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/dd/ba/a0e6866057fc0bbd17192925c1d63a3b85cf522965de9bc02364d08e5b84/lxml-4.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium==3.141.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Collecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/05/cf/ea245e52f55823f19992447b008bcbb7f78efc5960d77f6c34b5b45b36dd/soupsieve-2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: soupsieve, beautifulsoup4, idna, requests, lxml\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed beautifulsoup4-4.8.0 idna-2.8 lxml-4.5.0 requests-2.22.0 soupsieve-2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "bs4",
                  "idna",
                  "lxml",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M5LMtQz4ZYR",
        "colab_type": "text"
      },
      "source": [
        "Execute the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAL2Sfe2vI4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIBFctbfvK7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "60839ad3-01c9-4d5f-94de-1462096a2281"
      },
      "source": [
        "!python3 news_wrapper.py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"news_wrapper.py\", line 84, in <module>\n",
            "    final_output = main_script()\n",
            "  File \"news_wrapper.py\", line 75, in main_script\n",
            "    original_files = news_size()\n",
            "  File \"news_wrapper.py\", line 40, in news_size\n",
            "    news_object = open_pickle(news_path)\n",
            "  File \"news_wrapper.py\", line 14, in open_pickle\n",
            "    object_name = pickle.load(pickle_file)\n",
            "TypeError: a bytes-like object is required, not 'str'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6QiTB88vREs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "f0ad41a8-f080-4506-8910-52db6408c13b"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TknZbGhr0UMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}